{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proof of concept\n",
    "\n",
    "- using [PRAW](https://praw.readthedocs.io/en/latest/)\n",
    "- opens `data_in.csv` that should look like `id,annotation` for each line\n",
    "- grabs some metadata associated with submission and media object\n",
    "- dumps into csv file and downloads me\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://praw.readthedocs.io/en/latest/code_overview/models/submission.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Setup\n",
    "\n",
    "`cp settings.yaml.orig settings.yaml`\n",
    "\n",
    "add API credentials to `settings.yaml`\n",
    "\n",
    "generate these with [instructions](https://github.com/reddit-archive/reddit/wiki/OAuth2-Quick-Start-Example#first-steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: praw in /Users/tim/opt/anaconda3/lib/python3.7/site-packages (7.2.0)\n",
      "Requirement already satisfied: update-checker>=0.18 in /Users/tim/opt/anaconda3/lib/python3.7/site-packages (from praw) (0.18.0)\n",
      "Requirement already satisfied: prawcore<3,>=2 in /Users/tim/opt/anaconda3/lib/python3.7/site-packages (from praw) (2.0.0)\n",
      "Requirement already satisfied: websocket-client>=0.54.0 in /Users/tim/opt/anaconda3/lib/python3.7/site-packages (from praw) (1.0.1)\n",
      "Requirement already satisfied: requests>=2.3.0 in /Users/tim/opt/anaconda3/lib/python3.7/site-packages (from update-checker>=0.18->praw) (2.22.0)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /Users/tim/opt/anaconda3/lib/python3.7/site-packages (from requests>=2.3.0->update-checker>=0.18->praw) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/tim/opt/anaconda3/lib/python3.7/site-packages (from requests>=2.3.0->update-checker>=0.18->praw) (2019.9.11)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /Users/tim/opt/anaconda3/lib/python3.7/site-packages (from requests>=2.3.0->update-checker>=0.18->praw) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/tim/opt/anaconda3/lib/python3.7/site-packages (from requests>=2.3.0->update-checker>=0.18->praw) (1.24.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#Needs specific install\n",
    "#Only need to run once\n",
    "%pip install praw "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import yaml\n",
    "import pprint\n",
    "import csv\n",
    "from urllib.request import urlretrieve\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://api.reddit.com/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = yaml.load(open(\"settings.yaml\").read(), yaml.SafeLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Version 7.2.0 of praw is outdated. Version 7.3.0 was released Thursday June 17, 2021.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elibtronic\n"
     ]
    }
   ],
   "source": [
    "#AUTH\n",
    "\n",
    "reddit = praw.Reddit(\n",
    "    client_id = configs['CLIENT_ID'],\n",
    "    client_secret = configs['CLIENT_SECRET'],\n",
    "    user_agent = \"MemBot by u/elibtronic\",\n",
    "    username = configs['USERNAME'],\n",
    "    password = configs['PASSWORD'],\n",
    ")\n",
    "\n",
    "print(reddit.user.me())\n",
    "sr = reddit.subreddit(\"memes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#What fields does a PRAW submission object have?\n",
    "# https://praw.readthedocs.io/en/latest/code_overview/models/submission.html\n",
    "#sub = reddit.submission(id=post_ids[1])\n",
    "#print(sub.url)\n",
    "#print(sub.title)\n",
    "#print(sub)\n",
    "#print(sub.ups)\n",
    "#print(sub.num_comments)\n",
    "#print(sub.total_awards_received)\n",
    "#pprint.pprint(vars(sub))\n",
    "\n",
    "#with open(\"metadata_fields.txt\", \"w\") as m_file:\n",
    "#    pprint.pprint(vars(sub), m_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_ids = {}\n",
    "\n",
    "with open(\"data_in.csv\",newline=\"\", encoding='cp1252') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file)\n",
    "    next(csv_reader)\n",
    "    for row in csv_reader:\n",
    "        post_ids[row[0]] = row[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 / 200\n",
      "2 / 200\n",
      "3 / 200\n",
      "4 / 200\n",
      "5 / 200\n",
      "6 / 200\n",
      "7 / 200\n",
      "8 / 200\n",
      "9 / 200\n",
      "10 / 200\n",
      "11 / 200\n",
      "12 / 200\n",
      "13 / 200\n",
      "14 / 200\n",
      "15 / 200\n",
      "16 / 200\n",
      "17 / 200\n",
      "18 / 200\n",
      "19 / 200\n",
      "20 / 200\n",
      "21 / 200\n",
      "22 / 200\n",
      "23 / 200\n",
      "24 / 200\n",
      "25 / 200\n",
      "26 / 200\n",
      "27 / 200\n",
      "28 / 200\n",
      "29 / 200\n",
      "30 / 200\n",
      "31 / 200\n",
      "32 / 200\n",
      "33 / 200\n",
      "34 / 200\n",
      "35 / 200\n",
      "36 / 200\n",
      "37 / 200\n",
      "38 / 200\n",
      "39 / 200\n",
      "40 / 200\n",
      "41 / 200\n",
      "42 / 200\n",
      "43 / 200\n",
      "44 / 200\n",
      "45 / 200\n",
      "46 / 200\n",
      "47 / 200\n",
      "48 / 200\n",
      "49 / 200\n",
      "50 / 200\n",
      "51 / 200\n",
      "52 / 200\n",
      "53 / 200\n",
      "54 / 200\n",
      "55 / 200\n",
      "56 / 200\n",
      "57 / 200\n",
      "58 / 200\n",
      "59 / 200\n",
      "60 / 200\n",
      "61 / 200\n",
      "62 / 200\n",
      "63 / 200\n",
      "64 / 200\n",
      "65 / 200\n",
      "66 / 200\n",
      "67 / 200\n",
      "68 / 200\n",
      "69 / 200\n",
      "70 / 200\n",
      "71 / 200\n",
      "72 / 200\n",
      "73 / 200\n",
      "74 / 200\n",
      "75 / 200\n",
      "76 / 200\n",
      "77 / 200\n",
      "78 / 200\n",
      "79 / 200\n",
      "80 / 200\n",
      "81 / 200\n",
      "82 / 200\n",
      "83 / 200\n",
      "84 / 200\n",
      "85 / 200\n",
      "86 / 200\n",
      "87 / 200\n",
      "88 / 200\n",
      "89 / 200\n",
      "90 / 200\n",
      "91 / 200\n",
      "92 / 200\n",
      "93 / 200\n",
      "94 / 200\n",
      "95 / 200\n",
      "96 / 200\n",
      "97 / 200\n",
      "98 / 200\n",
      "99 / 200\n",
      "100 / 200\n",
      "101 / 200\n",
      "102 / 200\n",
      "103 / 200\n",
      "104 / 200\n",
      "105 / 200\n",
      "106 / 200\n",
      "107 / 200\n",
      "108 / 200\n",
      "109 / 200\n",
      "found a bad post! o9bk15\n",
      "110 / 200\n",
      "111 / 200\n",
      "112 / 200\n",
      "113 / 200\n",
      "114 / 200\n",
      "115 / 200\n",
      "116 / 200\n",
      "117 / 200\n",
      "118 / 200\n",
      "119 / 200\n",
      "120 / 200\n",
      "121 / 200\n",
      "122 / 200\n",
      "123 / 200\n",
      "124 / 200\n",
      "125 / 200\n",
      "126 / 200\n",
      "127 / 200\n",
      "128 / 200\n",
      "129 / 200\n",
      "130 / 200\n",
      "131 / 200\n",
      "132 / 200\n",
      "133 / 200\n",
      "134 / 200\n",
      "135 / 200\n",
      "136 / 200\n",
      "137 / 200\n",
      "138 / 200\n",
      "139 / 200\n",
      "140 / 200\n",
      "141 / 200\n",
      "142 / 200\n",
      "143 / 200\n",
      "144 / 200\n",
      "145 / 200\n",
      "146 / 200\n",
      "147 / 200\n",
      "148 / 200\n",
      "149 / 200\n",
      "150 / 200\n",
      "151 / 200\n",
      "152 / 200\n",
      "153 / 200\n",
      "154 / 200\n",
      "155 / 200\n",
      "156 / 200\n",
      "157 / 200\n",
      "158 / 200\n",
      "159 / 200\n",
      "160 / 200\n",
      "161 / 200\n",
      "162 / 200\n",
      "163 / 200\n",
      "164 / 200\n",
      "165 / 200\n",
      "166 / 200\n",
      "167 / 200\n",
      "168 / 200\n",
      "169 / 200\n",
      "170 / 200\n",
      "171 / 200\n",
      "172 / 200\n",
      "173 / 200\n",
      "174 / 200\n",
      "175 / 200\n",
      "176 / 200\n",
      "177 / 200\n",
      "178 / 200\n",
      "179 / 200\n",
      "180 / 200\n",
      "181 / 200\n",
      "182 / 200\n",
      "183 / 200\n",
      "184 / 200\n",
      "185 / 200\n",
      "186 / 200\n",
      "187 / 200\n",
      "188 / 200\n",
      "189 / 200\n",
      "190 / 200\n",
      "191 / 200\n",
      "192 / 200\n",
      "193 / 200\n",
      "194 / 200\n",
      "195 / 200\n",
      "196 / 200\n",
      "197 / 200\n",
      "198 / 200\n",
      "199 / 200\n",
      "200 / 200\n",
      "\n",
      "\n",
      "Completed the download of metadata and media\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Flip to true to download media as well\n",
    "ALSO_DOWNLOAD = True\n",
    "\n",
    "progress = 0\n",
    "total = str(len(post_ids))\n",
    "\n",
    "#Write out entries to CSV logfiel\n",
    "lf = datetime.now().strftime(\"%Y_%m_%d_%H_%M\") + \"_memlog.csv\"\n",
    "with open(lf,'w',newline='') as logfile:\n",
    "    writer = csv.writer(logfile)\n",
    "    \n",
    "    header = []\n",
    "    header.append(\"post_id\")\n",
    "    header.append(\"annotation\")\n",
    "    header.append(\"date_posted\")\n",
    "    header.append(\"permalink\")\n",
    "    header.append(\"title\")\n",
    "    header.append(\"ups\")\n",
    "    header.append(\"num_comments\")\n",
    "    header.append(\"total_awards\")\n",
    "    header.append(\"media_uri\")\n",
    "    \n",
    "    writer.writerow(header)\n",
    "    \n",
    "    for post in post_ids.keys():\n",
    "        \n",
    "        sub = reddit.submission(id=post)\n",
    "        \n",
    "        tstamp = datetime.fromtimestamp(sub.created_utc).strftime(\"%m/%d/%Y\")\n",
    "        f_name = \"meme_download/\"+str(post)+\"_\"+sub.url.split(\"/\")[-1]\n",
    "        \n",
    "        if ALSO_DOWNLOAD:\n",
    "            try:\n",
    "                urlretrieve(sub.url,f_name)\n",
    "            except:\n",
    "                print(\"found a bad post! \"+ str(post))\n",
    "        \n",
    "        line = []\n",
    "        line.append(post) #unique ID\n",
    "        line.append(post_ids[post])\n",
    "        line.append(tstamp) #Date Posted\n",
    "        line.append(sub.permalink) #URI (minus domain name)\n",
    "        line.append(sub.title) #Title\n",
    "        line.append(sub.ups) #Ups\n",
    "        line.append(sub.num_comments) # Number of comments\n",
    "        line.append(sub.total_awards_received) # Number of awards\n",
    "        line.append(sub.url) #URI to media file\n",
    "        #print(line)\n",
    "        \n",
    "        writer.writerow(line)\n",
    "        progress +=1\n",
    "        print(str(progress) + \" / \" + total)\n",
    "        \n",
    "print(\"\\n\\nCompleted the download of metadata and media\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
